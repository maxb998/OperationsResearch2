\chapter{Coding Approaches}

\section{Approaches to Cost Function}

When talking about TSP, the distance between nodes can be seen as the cost of the edges themselves.
These type of distances are defined by TSPLIB\cite{tsplib} and can be computed or precomputed and given as input inside the .tsp files, if they are not specified to begin with.
In this project only instances where the cost of the edges is represented by the distance between nodes will be considered, hence all the instances are made of a fully connected graph.
Since the edge costs are frequently accessed during the execution of the algorithms studied in this paper, the method to retrieve them must be as fast as possible.
It is possible to differentiate between two main categories of methods for retrieving edge costs.

The first one consist in computing the distance each time it is needed, while the second method involves computing all the distances at the beginning and storing each edge value inside the memory.
While the first is the simpleset and most straightforward implementation, it does come with a major drawback: speed.
Indeed there are more than a couple distance function defined by TSPLIB, almost every instance used during this project development included the need to compute the square root at some point.
The square root operation is an operation that is computationally intesive for computers that usually require iterative algorithms to compute.
Nowadays most modern computer architectures include an hardware dedicated square root instruction, which is many times faster compared to the interative algorithm, yet still takes a lot of time to execute.

The second category of methods eliminates this problem, since allowing to compute all cost at the start completely removes the need to use any kind of cost function down the line.
On paper this method is indeed the one which allows for the greatest speedup, but, unfortunatly that is not always the case.
Since the graphs under consideration are all fully connected, the number of edges increases in a quadratic relationship to the number of nodes.
Because of this the amount of memory needed to cache the cost of all edges is in the order of $O(n^2)$, as an example 137MB of memory is needed to store the edge costsof an instance with 6000 nodes.
Although it is still a small amount of memory compared to the amount of memory modern computers are equipped with, it is too much to fit a normal personal computer CPU cache.
It's well known that CPU cache is significantly faster than regular system memory, so being unable to fit the cost matrix inside the cache can negatively impact performance.
Cache misses occur when the data the CPU needs is not found in the fast-access cache memory and must be retrieved from the slower main memory.
This delay happens because the CPU has to pause its current operations to fetch the required data, leading to increased latency and reduced overall performance.

Wanting to obtain faster speed than both of the simple methods explained above, the focus fall on optimizing one of them.
Since the second category methods are harder to optimize, more focus was given in finding a way to compute edge cost faster, thus optimizing the first category of methods.
The lingering issue lies in the time consumed by the square root operation, which can be streamlined through various optimizations like leveraging a lookup table.
To address this, the approach utilized SIMD instructions known as AVX.
AVX (Advanced Vector Extensions) are a set of CPU instructions designed to perform parallel operations on multiple data elements simultaneously.
They enable faster processing by allowing a single instruction to operate on multiple data points, enhancing the efficiency of tasks such as mathematical computations and data manipulation.
Supporting operations between vectors of the size of 256 bits, AVX allows computing eight distance values, in floating-point 32-bit precision, simultaneously, enhancing computational efficiency.
Having said that, the process of coding an algorithm to make use of AVX is a bit more complicated, as well as the fact that not all algorithms can be optimized because they cannot be executed in a vectorized manner (for example Simulated Annealing).

\subsection{Performance Comparison}

This section will put to light the difference in execution speed between the above mentioned methods.

\section{Multithreading}

Multithreading is a programming technique that allows a computer program to perform multiple tasks concurrently, by dividing its execution into smaller threads of execution that can run independently.
In this project, multithreading is exploited by running the same code on different threads while using at random different data.
This is done by means of random seed, which means, giving to each thread running a different seed value so that it may find different solutions from the other running threads.
During this process the threads share between them the globally best solution found up to that point, so that, at the end of the execution, that solution will be given as output.

